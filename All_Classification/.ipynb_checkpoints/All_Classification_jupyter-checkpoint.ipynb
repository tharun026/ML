{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "import pickle\n",
    "import operator\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#pre process\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# algos\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# model selection\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# analysis\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load The Dataset\n",
    "url = \"H:\\Imarticus\\All_Classification\\imdb-sentiments\\data.csv\"\n",
    "features = ['text','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data):\n",
    "    print(\"vectorizing data\")\n",
    "    if type(data)==str: data=data.split()\n",
    "    cdata=set(data)\n",
    "    vector_map={}; i=0\n",
    "    for c in cdata:\n",
    "        vector_map[i]=c; i=i+1\n",
    "    #print(vector_map)\n",
    "    return vector_map\n",
    "\n",
    "\n",
    "def sort_by_value(dictx):\n",
    "    print(\"sorting by value\")\n",
    "    dicty=sorted(dictx.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return dicty\n",
    "    \n",
    "\n",
    "def tex2vec(X):\n",
    "    print(\"text2vec\")\n",
    "    xlist=[]\n",
    "    if type(X)==str: X=[X]; print(\"txt2vec--->string\")\n",
    "    if type(X)==list: xlist=X; print(\"txt2vec--->list\")\n",
    "    if type(X)==np.ndarray:\n",
    "        for i in range(len(X)): xlist.append(X[i][0])\n",
    "        print(\"txt2vec--->ndarray\"); #print(X[i][0]); return\n",
    "        \n",
    "    vectorizer = TfidfVectorizer(min_df=0.001, max_df=1.0)\n",
    "    trainvectors = vectorizer.fit_transform(xlist)\n",
    "    return trainvectors\n",
    "\n",
    "#tv = tex2vec([\"i am on the train and travelling to home town\", \"my home town is traditionally rich\"]); print(tv) \n",
    "\n",
    "def get_XY(url, vectorize, features):\n",
    "    print(\"getXY\")\n",
    "    dataset = pandas.read_csv(url, names=features, encoding=\"ISO-8859-1\"); #print(dataset)\n",
    "    if vectorize==1:\n",
    "        h=list(dataset.columns.values)[0]; #print(h)\n",
    "        dataset[h] = dataset[h].values.astype('U')\n",
    "    array = dataset.values; #print(array[0])\n",
    "    n = len(array[0]); #print(\"len--->\", n)\n",
    "    X = array[:,0:n-1]\n",
    "    Y = array[:,n-1]\n",
    "    return X, Y\n",
    "\n",
    "#X, Y = get_XY(url, 0, features); print(X[:5], Y[:5])\n",
    "\n",
    "\n",
    "\n",
    "def pca_prog(url, features, n):\n",
    "    print(\"pca_prog\")\n",
    "    X, Y =  get_XY(url, 0, features)\n",
    "    X, Y = X[1:], Y[1:]\n",
    "    pca = PCA(n_components=n)\n",
    "    fit = pca.fit(X);\n",
    "    print(fit.explained_variance_ratio_)\n",
    "#    print(fit.singular_values_)\n",
    "    xfit = fit.components_; #print(xfit[:5])\n",
    "    return xfit\n",
    "\n",
    "#pca_prog(url, features, 4)\n",
    "\n",
    "\n",
    "def rfe_prog(url, features, n):\n",
    "    print(\"rfe_prog\")\n",
    "    X, Y =  get_XY(url, 0, features)\n",
    "    X, Y = X[1:], Y[1:]\n",
    "    model = LogisticRegression()\n",
    "    rfe = RFE(model, n)\n",
    "    fit = rfe.fit(X,Y)\n",
    "    xfit = fit.ranking_; print(xfit)\n",
    "    return xfit\n",
    "\n",
    "#rfe_prog(url, features, 1)\n",
    "\n",
    "def ETC_prog(url, features, n):\n",
    "    print(\"ETC prog\")\n",
    "    X, Y =  get_XY(url, 0, features)\n",
    "    X, Y = X[1:], Y[1:]\n",
    "    model = ExtraTreesClassifier()\n",
    "    model.fit(X, Y)\n",
    "    f_imp = model.feature_importances_; print(f_imp)\n",
    "    print(X[:2])\n",
    "    return f_imp\n",
    "\n",
    "#ETC_prog(url, features, 4)\n",
    "\n",
    "def top_fits(X, n, f_imp):\n",
    "    print(\"top fits\")\n",
    "    importance={}\n",
    "    for i in range(len(f_imp)):\n",
    "        importance[i]=round(f_imp[i],3)\n",
    "    importance = sort_by_value(importance); print(importance)\n",
    "    \n",
    "    XT = X.transpose(); xNew=[]; c=0\n",
    "    for k, v in importance:\n",
    "        xNew.append(XT[k])\n",
    "        if c>=n: break\n",
    "        c=c+1\n",
    "    xNew = np.array(xNew)\n",
    "    X = xNew.transpose()\n",
    "    return X\n",
    "\n",
    "#top_fits(X, n, f_imp)\n",
    "def summarize(url, features):\n",
    "    print(\"summarize\")\n",
    "    dataset = pandas.read_csv(url, names=features)\n",
    "    newdata = pandas.DataFrame([])\n",
    "    for col in dataset.columns:\n",
    "        if col==\"class\": continue\n",
    "        newdata[col]=np.log(dataset[col])\n",
    "\n",
    "    #Summarize the Dataset\n",
    "    Summary={}\n",
    "    Summary['Shape']=dataset.shape; print(Summary['Shape'])\n",
    "    Summary['Describe']=dataset.describe(); print(Summary['Describe'])\n",
    "    Summary['Groups']=dataset.groupby('class').size().to_json(); print(Summary['Groups'])\n",
    "      \n",
    "#    Data Visualization\n",
    "#    dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False); plt.savefig(\"box.jpg\"); plt.show()\n",
    "#    dataset.hist(); plt.savefig(\"hist.jpg\"); plt.show()\n",
    "#    newdata.hist(); plt.show()\n",
    "#    scatter_matrix(dataset); plt.savefig(\"scatter.jpg\"); plt.show()\n",
    "\n",
    "#summarize(url, features) \n",
    " \n",
    "def get_models():\n",
    "    print(\"get_models\")\n",
    "    models = {}\n",
    "    models['LogR'] = LogisticRegression()\n",
    "    models['LDA'] = LinearDiscriminantAnalysis()\n",
    "    models['KNN'] = KNeighborsClassifier()\n",
    "    models['DTC'] = DecisionTreeClassifier(criterion=\"gini\")\n",
    "    models['NBC'] = GaussianNB()\n",
    "    models['SVC'] = SVC()\n",
    "    models['RFC'] = RandomForestClassifier()\n",
    "    models['MLP'] = MLPClassifier()\n",
    "    models['GBC'] = GradientBoostingClassifier()\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(url, features, vectorize):\n",
    "    print(\"compare\")\n",
    "    X, Y = get_XY(url, vectorize, features) \n",
    "    if vectorize==1: X = tex2vec(X); X = X.toarray(); #print(X)\n",
    "    \n",
    "    #create validation set\n",
    "    validation_size = 0.20\n",
    "    seed = 7\n",
    "    Xt, Xv, Yt, Yv = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
    "    \n",
    "    #Test options and evaluation metric\n",
    "    scoring = 'accuracy'\n",
    "\n",
    "    models = get_models()\n",
    "    # evaluate each model in turn\n",
    "    results = []\n",
    "    model_names = []\n",
    "    model_list = {}\n",
    "    compare_list = {}\n",
    "    for name, model in models.items():\n",
    "        kfold = model_selection.KFold(n_splits=2, random_state=seed)\n",
    "        cv_results = model_selection.cross_val_score(model, Xt, Yt, cv=kfold, scoring=scoring)\n",
    "        results.append(cv_results)\n",
    "        model_names.append(name)\n",
    "        model_list[model]=cv_results.mean()\n",
    "        compare_list[name]=[\" Mean: \"+str(round(cv_results.mean(),2)), \"  Std: \"+str(round(cv_results.std(),2))]\n",
    "        print(name, ':', cv_results.mean(), cv_results.std())\n",
    "    #print(model_names)\n",
    "    model_dict=sort_by_value(model_list); print(model_dict)\n",
    "    final_model=model_dict[0][0]; print('final_model: ',final_model)\n",
    "\n",
    "    #Compare Algorithms\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Algorithm Comparison')\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot(results)\n",
    "    ax.set_xticklabels(model_names)\n",
    "    plt.savefig('comparison.jpg'); plt.show()\n",
    "    print(compare_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    print(\"plot_learning_Curve\")\n",
    "    plt.figure(); print(title); print(y)\n",
    "    plt.title(title)\n",
    "    if ylim is not None: plt.ylim(*ylim); print(1)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, verbose=2)\n",
    "    print(2)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    print(3)\n",
    "    plt.grid()\n",
    "    print(4)\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    print(5)\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    print(6)\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    print(7)\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    print(8)\n",
    "    plt.legend(loc=\"best\"); plt.show()\n",
    "    print(\"end of learning curve\")\n",
    "\t#plt.savefig(\"learning_curve.jpg\"); plt.show()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(url, features, model_key, vectorize, model_name):\n",
    "    print(\"train\")\n",
    "    model_dict=get_models()\n",
    "    final_model=model_dict[model_key]; print('final_model--->', final_model)\n",
    "    X, Y = get_XY(url, vectorize, features); print(\"input-->\", X[0]); print(\"label-->\", Y[0])\n",
    "    if vectorize==1: X = tex2vec(X); X = X.toarray(); print(X.shape)\n",
    "    \n",
    "    # selecting importanct features\n",
    "    #n = 10\n",
    "    #X = pca_prog(url, features, n)\n",
    "    #X = ETC_prog(url, features, n)\n",
    "    \n",
    "    print(\"create validation set\")\n",
    "    validation_size = 0.20\n",
    "    seed = 7\n",
    "    Xt, Xv, Yt, Yv = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
    "\n",
    "##    fpr, tpr, th = roc_curve(Yt, Yv)\n",
    "##    roc_auc = auc(fpr, tpr); print('roc_auc-->', roc_auc)\n",
    "    \n",
    "    ## Make predictions on validation dataset\n",
    "    Yt = Yt.reshape(Yt.size, 1); print(\"reshaped Yt\")\n",
    "    final_model.fit(Xt, Yt); print(\"fitting model\")\n",
    "    predictions = final_model.predict(Xv)\n",
    "    score = accuracy_score(Yv, predictions)\n",
    "    report = classification_report(Yv, predictions)\n",
    "    matrix = confusion_matrix(Yv, predictions)\n",
    "    print('Accuracy: ', score); \n",
    "    print(\"\"); print(report); print(\"\"); print(matrix)\n",
    "    pickle.dump(final_model, open('models/'+model_name+'.pickle','wb')); print(\"Training Completed\")\n",
    "    \n",
    "    title = \"Learning Curves - \"+str(model_key); print(title)\n",
    "    # Cross validation with 2 iterations to get smoother mean test and train\n",
    "    # score curves, each time with 20% data randomly selected as a validation set.\n",
    "    cv = ShuffleSplit(n_splits=2, test_size=0.2, random_state=0); print(\"cross validation complete\")\n",
    "#    Y = Y.reshape(150,1)\n",
    "    y=Y; #print(np.bincount(y))\n",
    "    plot_learning_curve(final_model, title, X, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "get_models\n",
      "final_model---> LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "getXY\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-f5a267d3c7bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvectorize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-54-97ba03e21d31>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(url, features, model_key, vectorize, model_name)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mfinal_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'final_model--->'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_XY\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"input-->\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"label-->\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtex2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-4c6e636231fd>\u001b[0m in \u001b[0;36mget_XY\u001b[1;34m(url, vectorize, features)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m \u001b[1;31m#print(h)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'U'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m;\u001b[0m \u001b[1;31m#print(array[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[1;31m#print(\"len--->\", n)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_key = \"LogR\"\n",
    "vectorize = 1\n",
    "model_name = model_key\n",
    "train(url, features, model_key, vectorize, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
